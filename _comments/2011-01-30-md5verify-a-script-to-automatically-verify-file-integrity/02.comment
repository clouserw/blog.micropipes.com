post_id: /2011/01/30/md5verify-a-script-to-automatically-verify-file-integrity
name: db48x
date: '2011-01-31 04:06:21 -0800'
comment: "This is really cool, but inadequately paranoid. For one thing, errors
  on hard drives (and in memory, hard drive controllers, etc) are distributed randomly,
  which means that it could be the hash which has the error, not the file that was
  hashed. Also, you can still lose whole clusters/inodes or even whole files if
  you get an error in the directory entries.\r\n\r\nWhat you need is hierarchical
  hashes. Hash the files, and store that like you do currently. Then hash all the
  metadata about those files (including the list of hashes) and store that in the
  parent directory. Do this recursively by depth-first traversal. Once you're done,
  the hashes in the top level directories monitor the contents of the lower level
  ones, allowing you to detect errors in them. Of course, don't forget to recursively
  update the hashes when you add or remove files, or change their contents.\r\n\r\nAlso,
  I would suggest that rather than writing your own implementation in python, you
  should look into using ZFS; it does all this for you, plus it can correct the
  errors it finds, not merely detect them. http://en.wikipedia.org/wiki/ZFS#Data_Integrity.
  I've been using zfs-fuse for a while now, and I really like it. It's not very
  fast (which is unfortunate, since I put my home directories into my ZFS pool),
  but it's perfect for archiving important data."

